{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Linear Regression with PyTorch\n\n\n\n![linear-regression-training-data](https://i.imgur.com/6Ujttb4.png)\n\nI\n\nThe *learning* part of linear regression is to figure out a set of weights `w11, w12,... w23, b1 & b2` by looking at the training data, to make accurate predictions for new data (i.e. to predict the yields for apples and oranges in a new region using the average temperature, rainfall and humidity). This is done by adjusting the weights slightly many times to make better predictions, using an optimization technique called *gradient descent*."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport torch","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Input (temp, rainfall, humidity)\ninputs = np.array([[73, 67, 43], \n                   [91, 88, 64], \n                   [87, 134, 58], \n                   [102, 43, 37], \n                   [69, 96, 70]], dtype='float32')","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Targets (apples, oranges)\ntargets = np.array([[56, 70], \n                    [81, 101], \n                    [119, 133], \n                    [22, 37], \n                    [103, 119]], dtype='float32')","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert inputs and targets to tensors\ninputs = torch.from_numpy(inputs)\ntargets = torch.from_numpy(targets)\nprint(inputs)\nprint(targets)","execution_count":27,"outputs":[{"output_type":"stream","text":"tensor([[ 73.,  67.,  43.],\n        [ 91.,  88.,  64.],\n        [ 87., 134.,  58.],\n        [102.,  43.,  37.],\n        [ 69.,  96.,  70.]])\ntensor([[ 56.,  70.],\n        [ 81., 101.],\n        [119., 133.],\n        [ 22.,  37.],\n        [103., 119.]])\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Linear regression model from scratch\n\nThe weights and biases (`w11, w12,... w23, b1 & b2`) can also be represented as matrices, initialized as random values. The first row of `w` and the first element of `b` are used to predict the first target variable i.e. yield of apples, and similarly the second for oranges."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Weights and biases\nw = torch.randn(2, 3, requires_grad=True)\nb = torch.randn(2, requires_grad=True)\nprint(w)\nprint(b)","execution_count":28,"outputs":[{"output_type":"stream","text":"tensor([[-0.2318, -0.3457,  0.6827],\n        [ 0.4089,  0.2843,  0.2326]], requires_grad=True)\ntensor([ 0.2932, -0.7258], requires_grad=True)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model(x):\n    return x @ w.t() + b","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate predictions\npreds = model(inputs)\nprint(preds)","execution_count":30,"outputs":[{"output_type":"stream","text":"tensor([[-10.4397,  58.1673],\n        [ -7.5368,  76.3803],\n        [-26.6092,  86.4257],\n        [-12.9614,  61.8064],\n        [ -1.1062,  71.0550]], grad_fn=<AddBackward0>)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compare with targets\nprint(targets)","execution_count":31,"outputs":[{"output_type":"stream","text":"tensor([[ 56.,  70.],\n        [ 81., 101.],\n        [119., 133.],\n        [ 22.,  37.],\n        [103., 119.]])\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Loss function\n We can compare the model's predictions with the actual targets, using the following method:\n\n* Calculate the difference between the two matrices (`preds` and `targets`).\n* Square all elements of the difference matrix to remove negative values.\n* Calculate the average of the elements in the resulting matrix.\n\nThe result is a single number, known as the **mean squared error** (MSE)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# MSE loss\ndef mse(t1, t2):\n    diff = t1 - t2\n    return torch.sum(diff * diff) / diff.numel()","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute loss\nloss = mse(preds, targets)\nprint(loss)","execution_count":33,"outputs":[{"output_type":"stream","text":"tensor(5134.4829, grad_fn=<DivBackward0>)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute gradients\nloss.backward()","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gradients for weights\nprint(w)\nprint(w.grad)","execution_count":35,"outputs":[{"output_type":"stream","text":"tensor([[-0.2318, -0.3457,  0.6827],\n        [ 0.4089,  0.2843,  0.2326]], requires_grad=True)\ntensor([[-7264.8682, -8650.3730, -5109.9209],\n        [-1586.8176, -2547.2639, -1444.8171]])\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Before we proceed, we reset the gradients to zero by calling `.zero_()` method. We need to do this, because PyTorch accumulates, gradients i.e. the next time we call `.backward` on the loss, the new gradient values will get added to the existing gradient values, which may lead to unexpected results."},{"metadata":{"trusted":true},"cell_type":"code","source":"w.grad.zero_()\nb.grad.zero_()\nprint(w.grad)\nprint(b.grad)","execution_count":36,"outputs":[{"output_type":"stream","text":"tensor([[0., 0., 0.],\n        [0., 0., 0.]])\ntensor([0., 0.])\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Adjust weights and biases using gradient descent\n\nWe'll reduce the loss and improve our model using the gradient descent optimization algorithm, which has the following steps:\n\n1. Generate predictions\n\n2. Calculate the loss\n\n3. Compute gradients w.r.t the weights and biases\n\n4. Adjust the weights by subtracting a small quantity proportional to the gradient\n\n5. Reset the gradients to zero\n\nLet's implement the above step by step."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate predictions\npreds = model(inputs)\nprint(preds)","execution_count":37,"outputs":[{"output_type":"stream","text":"tensor([[-10.4397,  58.1673],\n        [ -7.5368,  76.3803],\n        [-26.6092,  86.4257],\n        [-12.9614,  61.8064],\n        [ -1.1062,  71.0550]], grad_fn=<AddBackward0>)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate the loss\nloss = mse(preds, targets)\nprint(loss)","execution_count":38,"outputs":[{"output_type":"stream","text":"tensor(5134.4829, grad_fn=<DivBackward0>)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute gradients\nloss.backward()\nprint(w.grad)\nprint(b.grad)","execution_count":39,"outputs":[{"output_type":"stream","text":"tensor([[-7264.8682, -8650.3730, -5109.9209],\n        [-1586.8176, -2547.2639, -1444.8171]])\ntensor([-87.9307, -21.2330])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adjust weights & reset gradients\nwith torch.no_grad():\n    w -= w.grad * 1e-5\n    b -= b.grad * 1e-5\n    w.grad.zero_()\n    b.grad.zero_()","execution_count":40,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n* We use `torch.no_grad` to indicate to PyTorch that we shouldn't track, calculate or modify gradients while updating the weights and biases. \n\n* We multiply the gradients with a *learning rate* of the algorithm.\n\n* After we have updated the weights, we reset the gradients back to zero, to avoid affecting any future computations."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(w)\nprint(b)","execution_count":41,"outputs":[{"output_type":"stream","text":"tensor([[-0.1592, -0.2592,  0.7338],\n        [ 0.4247,  0.3097,  0.2470]], requires_grad=True)\ntensor([ 0.2941, -0.7256], requires_grad=True)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate loss\npreds = model(inputs)\nloss = mse(preds, targets)\nprint(loss)","execution_count":42,"outputs":[{"output_type":"stream","text":"tensor(3633.2859, grad_fn=<DivBackward0>)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Train for multiple epochs\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train for 500 epochs\nfor i in range(500):\n    preds = model(inputs)\n    loss = mse(preds, targets)\n    loss.backward()\n    with torch.no_grad():\n        w -= w.grad * 1e-5\n        b -= b.grad * 1e-5\n        w.grad.zero_()\n        b.grad.zero_()","execution_count":50,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate loss\npreds = model(inputs)\nloss = mse(preds, targets)\nprint(loss)","execution_count":51,"outputs":[{"output_type":"stream","text":"tensor(1.6790, grad_fn=<DivBackward0>)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predictions\npreds","execution_count":52,"outputs":[{"output_type":"execute_result","execution_count":52,"data":{"text/plain":"tensor([[ 57.1435,  70.2951],\n        [ 82.7697,  99.8428],\n        [117.4487, 134.8602],\n        [ 20.7309,  37.5597],\n        [103.1616, 117.2373]], grad_fn=<AddBackward0>)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Targets\ntargets","execution_count":53,"outputs":[{"output_type":"execute_result","execution_count":53,"data":{"text/plain":"tensor([[ 56.,  70.],\n        [ 81., 101.],\n        [119., 133.],\n        [ 22.,  37.],\n        [103., 119.]])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}